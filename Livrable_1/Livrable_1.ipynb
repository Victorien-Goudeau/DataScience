{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Livrable 1 : Classification Binaire\n",
    "|Erwan Martin|Thibaut Liger-Hellard|Arnaud Maturel|Guillaume Le Cocguen|Victorien Goudeau|\n",
    "|------------|---------------------|--------------|--------------------|-----------------|\n",
    "\n",
    "# **Contexte**:\n",
    "L'entreprise TouNum, spécialisée dans la numérisation de documents, collabore avec des spécialistes en Data Science de CESI pour développer une solution de Machine Learning capable de générer automatiquement des légendes pour les images numérisées. Ce projet vise à enrichir leur offre de services en répondant aux besoins de clients ayant d'importantes quantités de données à classer. Le défi inclut le nettoyage des images de qualité variable et la distinction entre photos et autres types d'images avant l'analyse. L'approche utilisera des technologies avancées telles que les réseaux de neurones convolutifs (CNN) et récurrents (RNN), en s'appuyant sur Python et des librairies spécialisées. Un prototype est attendu dans cinq semaines, suivi d'une présentation détaillée et d'une discussion sur l'intégration et la maintenance de la solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# **Objectif**:\n",
    "\n",
    "L'objectif de ce projet est de développer un modèle de classification binaire capable de distinguer les images de photos des autres types d'images. Le modèle sera entraîné sur un jeu de données étiqueté et évalué sur un jeu de données de test distinct. Les performances du modèle seront mesurées en termes de précision, de rappel et de F1-score. Le modèle final sera évalué sur un ensemble de données non étiqueté pour tester sa capacité à généraliser à de nouvelles images.\n",
    "\n",
    "# **Données**:\n",
    "\n",
    "Notre jeu de données contient plusieurs milliers d'images de différentes tailles et qualités. Ces images ne sont pas étiquetées, mais les images étant bien archivées, nous allons pouvoir les étiquetter manuellement.\n",
    "\n",
    "# **Défis techniques**:\n",
    "\n",
    "- Étiquettage des données: Les images ne sont pas étiquetées, il faudra donc le faire manuellement pour entraîner le modèle.\n",
    "\n",
    "- Qualité des images: Les images sont de qualité variable, ce qui peut affecter les performances du modèle.\n",
    "\n",
    "- Modèle de classification: Le modèle doit être capable de distinguer les images de photos des autres types d'images avec une précision élevée. Il faudra peut-être expérimenter avec différents types de modèles pour obtenir les meilleures performances.\n",
    "\n",
    "- Évaluation du modèle: Le modèle sera évalué sur un ensemble de données de test distinct pour mesurer sa précision, son rappel et son F1-score. Il sera également évalué sur un ensemble de données non étiqueté pour tester sa capacité à généraliser à de nouvelles images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# **Preprocessing**\n",
    "\n",
    "Afin de répondre au besoin de la société TouNum, nous allons dans un premier temps traiter les images, c'est à dire avoir une phase de `preprocessing` pour les rendre exploitables par notre modèle. Nous utiliserons de nombreuses bibliothèques python comme `tensorflow`, `scikit-learn` et `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import os\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "import tensorflow as tf\n",
    "import tensorboard\n",
    "\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "#Setting gpu for limit memory\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    #Restrict Tensorflow to only allocate 6gb of memory on the first GPU\n",
    "   try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(gpus[0],\n",
    "       [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=9144)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "   except RuntimeError as e:\n",
    "       #virtual devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récupération du chemin des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\CESI\\A5\\datascience\\Projet\\data\n"
     ]
    }
   ],
   "source": [
    "filePath = open('../data/path.txt', \"r\")\n",
    "datapath = filePath.read()\n",
    "print(datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = []\n",
    "imgs = []\n",
    "target_label = 'Photo'\n",
    "labels_to_remove = [\"Dataset_L2\", \"Photo_2\"]\n",
    "csv_filepath = \"../data/dataset_L1.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Récupération des différents labels\n",
    "\n",
    "Comme mentionné précédemment, les images ne sont pas étiquetées. Nous allons donc devoir le faire manuellement grâce à l'archivage des images, plus précisément le nom des dossiers dans lesquels elles sont stockées. Nous allons donc récupérer tous les labels possibles en fonction des noms de dossier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABELS : ['Painting', 'Photo', 'Schematics', 'Sketch', 'Text']\n"
     ]
    }
   ],
   "source": [
    "def create_labels_list(datapath, labels_to_remove=None):\n",
    "    for i in os.listdir(path=datapath):\n",
    "        if os.path.isdir(os.path.join(datapath, i)):\n",
    "            labels.append(i)\n",
    "\n",
    "    if labels_to_remove:\n",
    "        for label in labels_to_remove:\n",
    "            try:\n",
    "                labels.remove(label)\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "    return labels\n",
    "\n",
    "labels = create_labels_list(datapath, labels_to_remove)\n",
    "print(f\"LABELS : {labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant créer un dataset en CSV, qui contiendra les chemins d'accès des images ainsi que leurs labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_dataset_csv(datapath, csv_filepath, target_label=\"Photo\"):\n",
    "    with open(csv_filepath, \"w\", encoding=\"utf-8\") as csv:\n",
    "        csv.write(\"pathname;label\\n\")\n",
    "\n",
    "        for label in labels:\n",
    "            if os.path.isdir(os.path.join(datapath, label)):\n",
    "                l = 1 if label == target_label else 0\n",
    "\n",
    "                for img in os.listdir(os.path.join(datapath, label)):\n",
    "                    if not(\".ini\" in img):\n",
    "                        csv.write(f\"{os.path.join(datapath, label, img)};{l}\\n\")\n",
    "\n",
    "create_dataset_csv(datapath, csv_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load du csv dans un DataFrame pandas (On ne charge que les chemins au lieu de charger plusieurs dizaine de miliers d'images):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pathname</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D:\\CESI\\A5\\datascience\\Projet\\data\\Painting\\pa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D:\\CESI\\A5\\datascience\\Projet\\data\\Painting\\pa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D:\\CESI\\A5\\datascience\\Projet\\data\\Painting\\pa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D:\\CESI\\A5\\datascience\\Projet\\data\\Painting\\pa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D:\\CESI\\A5\\datascience\\Projet\\data\\Painting\\pa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            pathname  label\n",
       "0  D:\\CESI\\A5\\datascience\\Projet\\data\\Painting\\pa...      0\n",
       "1  D:\\CESI\\A5\\datascience\\Projet\\data\\Painting\\pa...      0\n",
       "2  D:\\CESI\\A5\\datascience\\Projet\\data\\Painting\\pa...      0\n",
       "3  D:\\CESI\\A5\\datascience\\Projet\\data\\Painting\\pa...      0\n",
       "4  D:\\CESI\\A5\\datascience\\Projet\\data\\Painting\\pa...      0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(csv_filepath, sep=';')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par la suite nous allons changer la taille de nos images en 400 par 400 pour que notre modèle puisse prendre une taille d'image fixe. Prendre une taille pas trop grande nous permettra de rester raisonnables en terme de temps de traitement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_height, image_width = 400, 400\n",
    "test_size = 0.3\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Séparation en jeu de test et d'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Séparation des données en ensembles d'entraînement et de test\n",
    "train_df, test_df = train_test_split(data, test_size=test_size, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création d'un Generator\n",
    "\n",
    "Le generator n'est pas obligatoire, mais nous permet de ne charger en mémoire que le batch (un petit groupe d'images sur lequel le modèle s'entraîne) en cours. Ainsi, nos ordinateurs sont en capacité d'entraîner le modèle car nous avons des batchs de 32 images contre un total d'environ 40000. \n",
    "Pour ce faire, nous répertorions tous les chemins d'accès aux fichiers images dans un fichier nommé \"dataset_L1.csv\". Ainsi, nos batchs sont des batchs de 32 chemins d'accès, nettement moins volumineux que les images en elles-mêmes.\n",
    "La fonction load_image charge l'image vers laquelle nous envoie le chemin d'accès.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, image_paths, labels, batch_size):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_paths = self.image_paths[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "        batch_labels = self.labels[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "        \n",
    "        batch_images = []\n",
    "        valid_labels = []\n",
    "        for path, label in zip(batch_paths, batch_labels):\n",
    "            img = self.load_image(path)\n",
    "            if img is not None:\n",
    "                batch_images.append(img)\n",
    "                valid_labels.append(label)\n",
    "        \n",
    "        return tf.convert_to_tensor(batch_images), tf.convert_to_tensor(valid_labels)\n",
    "\n",
    "    def load_image(self, path):\n",
    "        try:\n",
    "            image = tf.io.read_file(path)\n",
    "            image = tf.image.decode_jpeg(image, channels=3)\n",
    "            image = tf.image.resize(image, [image_height, image_width])\n",
    "            return image\n",
    "        except tf.errors.InvalidArgumentError:\n",
    "            print(f\" \\n Attention : le fichier {path} n'est pas une image valide et sera ignoré.\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Création des générateurs pour les ensembles d'entraînement et de test\n",
    "train_generator = ImageGenerator(train_df['pathname'].tolist(), train_df['label'].tolist(), batch_size)\n",
    "test_generator = ImageGenerator(test_df['pathname'].tolist(), test_df['label'].tolist(), batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajout de Callback\n",
    "\n",
    "En cas de problème lors de l'exécution de nos Epoch, nous mettons en place des callbacks. Si notre modèle tend vers de l'overfitting, alors, grâce aux callbacks, on peut arrêter l'entraînement du modèle avant d'avoir terminé toutes nos Epochs. En cas d'arrêt inopiné pendant l'entraînement du modèle, nous mettons également en place un callback pour sauvegarder nos poids quand que le résultat d'une Epoch s'améliore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStoppingAtMinLoss(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n",
    "\n",
    "    Arguments:\n",
    "        patience: Number of epochs to wait after min has been hit. After this\n",
    "        number of no improvement, training stops.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patience=0):\n",
    "        super().__init__()\n",
    "        self.patience = patience\n",
    "        # best_weights to store the weights at which the minimum loss occurs.\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # The number of epoch it has waited when loss is no longer minimum.\n",
    "        self.wait = 0\n",
    "        # The epoch the training stops at.\n",
    "        self.stopped_epoch = 0\n",
    "        # Initialize the best as infinity.\n",
    "        self.best = np.Inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(\"val_loss\")\n",
    "        print(\"The average loss for epoch {} is {:7.2f} \".format(epoch, logs[\"loss\"])) #print the loss\n",
    "\n",
    "        if np.less(current, self.best):\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            # Record the best weights if current results is better (less).\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                print(\"Restoring model weights from the end of the best epoch.\")\n",
    "                self.model.set_weights(self.best_weights)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))\n",
    "\n",
    "\n",
    "#TENSORBOARD\n",
    "time = datetime.now()\n",
    "foldername = f\"./tensorboard/{time.day}_{time.month}_{time.year}_{time.hour}h{time.minute}\"\n",
    "\n",
    "tensorflowCallback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=foldername,\n",
    "    histogram_freq=0,\n",
    "    write_graph=True,\n",
    "    write_images=False,\n",
    "    write_steps_per_second=False,\n",
    "    update_freq='epoch',\n",
    "    profile_batch=0,\n",
    "    embeddings_freq=0,\n",
    "    embeddings_metadata=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Convolutional Neural Network (CNN)** #\n",
    "\n",
    "Les réseaux de neuronnes convolutifs sont très largement utilisés lors d'opération de traitement d'image, et ce pour plusieurs raisons. \n",
    "\n",
    "La convolution est une opération consistant à créer un filtre que nous déplacerons sur notre image d'entrée afin de n'en traiter qu'une partie. Le pas choisis pour déplacer notre filtre est notre **stride**\n",
    "\n",
    "![filtrage.png](attachment/filtrage.png)\n",
    "![padding.png](attachment/maxpool.png)\n",
    "\n",
    "Quand nous demandons à un algorithme de traiter une image ce dernier doit assimiler une grande quantité d'informations, l'image étant traiter comme un tableau multidimensionnel contenant plusieurs valeur représentant les couleurs affichées, la convolution permet d'alimenter notre algorithme avec des fragments de notre image d'entrée afin de réduire le nombre de dimensions à traiter. En utilisant une image moins volumineuse l'algorithme va produire une donnée moins dense qui va sera moins lourde à traiter par la suite.\n",
    "En multipliant les déplacement sur notre image l'algorithme va produire une nouvelle valeur qui sera représentative des informations contenu sur notre portion d'image. L'ensemble de ces valeurs une fois notre image initiale entièrement parcourue est notre **carte de caractéristique** ou **feature map**\n",
    "\n",
    "Une fois notre feature map constituée nous utilisons une **fonction d'activation** qui va avoir pour rôle d'introduire de la non-linéarité dans notre réseau, cela afin d'apporter une classification plus représentative de la compléxité de nos données.\n",
    "\n",
    "![function.png](attachment/function.png)\n",
    "\n",
    "Une fois cela fait, nous pouvons au besoin introduire du **padding** qui va modifier les dimensions de nos données d'entrées afin que nos données une fois traitées soient de même tailles que nos données d'entrées\n",
    "\n",
    "![padding.png](attachment/padding.png)\n",
    "\n",
    "Par la suite nous avons une étape d'**agrégation (pooling)** lors de laquelle **les dimensions de la feature map sont réduites**. Dans notre cas nous utiliserons l'agrégation maximale (max pooling). Cette étape est destinée à réduire la quantité de mémoire nécéssaire à la réalisation des calculs tout en conservant les caractéristiques les plus importantes de notre feature map\n",
    "\n",
    "Et pour finir, afin de pouvoir intégrer notre image retravaillée par convolution dans nos neurones nous devons **applatir (flat)** le résultat de nos convolutions\n",
    "\n",
    "![maxpool.png](attachment/flaten.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction du modèle\n",
    "\n",
    "Après avoir traité nos donnés, géré les erreurs et générer notre train et test set, on peut passer à la construction de notre modèle.\n",
    "\n",
    "Pour ne pas subir de `sur apprentissage`, on met en place du `dropout`, ou bien de l'`early stopping`. Le `dropout` est une méthode de régularisation, qui nous permet d'aléatoirement désactiver des neuronnes, en les réinitialisant à 0. Cette méthode va s'appliquer lors de chaque Epoch, afin de mieux répartir l'apprentissage entre les neuronnes.\n",
    "\n",
    "Pour éviter l'effet inverse, à savoir le `sous apprentissage`, il faut fournir à notre modèle un assez grand nombre de données. Si le dataset se trouve être trop petit, alors il faut mettre en place de la `data augmentation`. Cette dernière consiste en prendre une image du dataset pour en générer plusieurs, par le biais de rotations de l'image ou de zoom par exemple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction du modèle\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Rescaling(1./255),\n",
    "    tf.keras.layers.Conv2D(32, (5, 5), activation='relu', input_shape=(image_height, image_width, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(data['label'].unique()), activation='softmax')  # Assurez-vous que cela correspond au nombre de classes\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "725/905 [=======================>......] - ETA: 1:17 - loss: 0.4744 - accuracy: 0.7744 \n",
      " Attention : le fichier D:\\CESI\\A5\\datascience\\Projet\\data\\Painting\\painting_02662.jpg n'est pas une image valide et sera ignoré.\n",
      "905/905 [==============================] - ETA: 0s - loss: 0.4526 - accuracy: 0.7783The average loss for epoch 0 is    0.45 \n",
      "905/905 [==============================] - 547s 601ms/step - loss: 0.4526 - accuracy: 0.7783 - val_loss: 0.3371 - val_accuracy: 0.8164\n",
      "Epoch 2/10\n",
      "311/905 [=========>....................] - ETA: 4:18 - loss: 0.3549 - accuracy: 0.8041 \n",
      " Attention : le fichier D:\\CESI\\A5\\datascience\\Projet\\data\\Painting\\painting_02662.jpg n'est pas une image valide et sera ignoré.\n",
      "905/905 [==============================] - ETA: 0s - loss: 0.3461 - accuracy: 0.8164The average loss for epoch 1 is    0.35 \n",
      "905/905 [==============================] - 541s 597ms/step - loss: 0.3461 - accuracy: 0.8164 - val_loss: 0.3065 - val_accuracy: 0.8555\n",
      "Epoch 3/10\n",
      "370/905 [===========>..................] - ETA: 3:45 - loss: 0.3090 - accuracy: 0.8497 \n",
      " Attention : le fichier D:\\CESI\\A5\\datascience\\Projet\\data\\Painting\\painting_02662.jpg n'est pas une image valide et sera ignoré.\n",
      "905/905 [==============================] - ETA: 0s - loss: 0.3081 - accuracy: 0.8502The average loss for epoch 2 is    0.31 \n",
      "905/905 [==============================] - 542s 599ms/step - loss: 0.3081 - accuracy: 0.8502 - val_loss: 0.2903 - val_accuracy: 0.8736\n",
      "Epoch 4/10\n",
      " 23/905 [..............................] - ETA: 5:54 - loss: 0.2757 - accuracy: 0.8601 \n",
      " Attention : le fichier D:\\CESI\\A5\\datascience\\Projet\\data\\Painting\\painting_02662.jpg n'est pas une image valide et sera ignoré.\n",
      "905/905 [==============================] - ETA: 0s - loss: 0.2803 - accuracy: 0.8729The average loss for epoch 3 is    0.28 \n",
      "905/905 [==============================] - 557s 616ms/step - loss: 0.2803 - accuracy: 0.8729 - val_loss: 0.2754 - val_accuracy: 0.8791\n",
      "Epoch 5/10\n",
      "215/905 [======>.......................] - ETA: 4:47 - loss: 0.2545 - accuracy: 0.8906 \n",
      " Attention : le fichier D:\\CESI\\A5\\datascience\\Projet\\data\\Painting\\painting_02662.jpg n'est pas une image valide et sera ignoré.\n",
      "905/905 [==============================] - ETA: 0s - loss: 0.2520 - accuracy: 0.8893The average loss for epoch 4 is    0.25 \n",
      "905/905 [==============================] - 540s 597ms/step - loss: 0.2520 - accuracy: 0.8893 - val_loss: 0.2633 - val_accuracy: 0.8842\n",
      "Epoch 6/10\n",
      "360/905 [==========>...................] - ETA: 3:50 - loss: 0.2164 - accuracy: 0.9095 \n",
      " Attention : le fichier D:\\CESI\\A5\\datascience\\Projet\\data\\Painting\\painting_02662.jpg n'est pas une image valide et sera ignoré.\n",
      "905/905 [==============================] - ETA: 0s - loss: 0.2384 - accuracy: 0.9012The average loss for epoch 5 is    0.24 \n",
      "905/905 [==============================] - 541s 598ms/step - loss: 0.2384 - accuracy: 0.9012 - val_loss: 0.2749 - val_accuracy: 0.8781\n",
      "Epoch 7/10\n",
      "630/905 [===================>..........] - ETA: 1:56 - loss: 0.2065 - accuracy: 0.9133 \n",
      " Attention : le fichier D:\\CESI\\A5\\datascience\\Projet\\data\\Painting\\painting_02662.jpg n'est pas une image valide et sera ignoré.\n",
      "905/905 [==============================] - ETA: 0s - loss: 0.2067 - accuracy: 0.9129The average loss for epoch 6 is    0.21 \n",
      "905/905 [==============================] - 541s 598ms/step - loss: 0.2067 - accuracy: 0.9129 - val_loss: 0.2631 - val_accuracy: 0.8925\n",
      "Epoch 8/10\n",
      "273/905 [========>.....................] - ETA: 4:29 - loss: 0.1689 - accuracy: 0.9304 \n",
      " Attention : le fichier D:\\CESI\\A5\\datascience\\Projet\\data\\Painting\\painting_02662.jpg n'est pas une image valide et sera ignoré.\n",
      "905/905 [==============================] - ETA: 0s - loss: 0.1851 - accuracy: 0.9251The average loss for epoch 7 is    0.19 \n",
      "905/905 [==============================] - 540s 597ms/step - loss: 0.1851 - accuracy: 0.9251 - val_loss: 0.2664 - val_accuracy: 0.8904\n",
      "Epoch 9/10\n",
      "668/905 [=====================>........] - ETA: 1:40 - loss: 0.1551 - accuracy: 0.9331 \n",
      " Attention : le fichier D:\\CESI\\A5\\datascience\\Projet\\data\\Painting\\painting_02662.jpg n'est pas une image valide et sera ignoré.\n",
      "905/905 [==============================] - ETA: 0s - loss: 0.1556 - accuracy: 0.9344The average loss for epoch 8 is    0.16 \n",
      "Restoring model weights from the end of the best epoch.\n",
      "905/905 [==============================] - 538s 594ms/step - loss: 0.1556 - accuracy: 0.9344 - val_loss: 0.2637 - val_accuracy: 0.8920\n",
      "Epoch 00009: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f030b9a730>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entraînement du modèle\n",
    "model.fit(train_generator, validation_data=test_generator, epochs=10, callbacks=[tensorflowCallback, EarlyStoppingAtMinLoss(patience=2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f'./models/{time.day}_{time.month}_{time.year}_{time.hour}h{time.minute}.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choix du modèle\n",
    "Avec nos callbacks, nous avons notre courbe de loss et d'accuracy sur tensorflow:\n",
    "\n",
    "![Courbes Loss and Accuracy](attachment/tensorboard_loss_acc_graph.png)\n",
    "\n",
    "Ici, nous avons représenté nos données de **validation**. On peut y voir l'évaluation de l'accuracy par epoch, ainsi que de la loss par epoch. Tous les entrainements ne font pas le même nombre d'epochs du à l'early stopping.\n",
    "\n",
    "Voici les valeurs pour l'accuracy:  \n",
    "\n",
    "![Valeurs Accuracy](attachment/accuracy_legend.png)\n",
    "\n",
    "Voici les valeurs pour la loss:  \n",
    "\n",
    "![Valeurs Accuracy](attachment/loss_legend.png)\n",
    "\n",
    "On choisira donc l'entrainement du 12/04/2024 à 14h58, puisqu'on a la loss la plus basse sur celui ci, c'est le modèle qui généralise le mieux."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
